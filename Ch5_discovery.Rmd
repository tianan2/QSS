---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(lubridate)
library(modelr)
library(tm)
library(SnowballC)
library(wordcloud)
theme_set(theme_light())
options(scipen = 999)
```

```{r}
DIR_SOURCE <- system.file("extdata/federalist", package = "qss")
corpus_raw <- VCorpus(DirSource(directory = DIR_SOURCE, pattern = "fp"))
corpus_raw
```

```{r}
corpus_tidy <- tidy(corpus_raw, "corpus") %>% 
  select(id, text) %>% mutate(document = as.integer(str_extract(id, "\\d+"))) %>% 
  select(-id)

tokens <- corpus_tidy %>% 
  unnest_tokens(word, text, token = "word_stems") %>% 
  mutate(word = str_remove(word, "\\d+")) %>% 
  filter(word != "")

tokens <- tokens %>% anti_join(stop_words, by = "word") 
dtm <- count(tokens, document, word)
```

```{r}
# Topic discovery
dtm %>% filter(document == 12) %>% {
  wordcloud(.$word, .$n, max.words = 20) }

dtm %>% filter(document == 24) %>% {
  wordcloud(.$word, .$n, max.words = 20)
}

dtm <- dtm %>% bind_tf_idf(word, document, n) 

# 10 important terms for document 12
dtm %>% filter(document == 12) %>% 
  top_n(10, tf_idf)
# 10 important terms for document 24
dtm %>% filter(document == 24) %>% 
  top_n(10, tf_idf)
```

```{r}
# Another approach: k-means clustering
HAMILTON_ESSAYS <- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85)
dtm_hamilton <- filter(dtm, document %in% HAMILTON_ESSAYS)

km_out <- kmeans(cast_dtm(dtm_hamilton, document, word, tf_idf), centers = 4, nstart = 10)
km_out$iter

hamilton_words <-
  tibble(word = colnames(cast_dtm(dtm_hamilton, document, word, tf_idf)))

dim(km_out$centers)
km_out$centers

hamilton_words <- bind_cols(hamilton_words, as_tibble(t(km_out$centers)))
hamilton_words
```

```{r}
# Find the top 10 words of each centeroid
top_words_cluster <- hamilton_words %>% 
  gather(cluster, value, -word) %>% 
  group_by(cluster) %>% 
  top_n(10, value) 

# Print out the top words using for loop
for (i in 1:4) {
  cat("Cluster", i, ": ", 
      str_c(top_words_cluster$word[top_words_cluster$cluster == i], collapse = ", "), 
      "\n"
  )
}

# Different approach by using kable
top_words_cluster %>% 
  summarize(top_words = str_c(word, collapse = ", ")) %>% 
  kableExtra::kable()
```

```{r}
HAMILTON_ESSAYS <- c(1, 6:9, 11:13, 15:17, 21:36, 59:61, 65:85)
MADISON_ESSAYS <- c(10, 14, 37:48, 58)
JAY_ESSAYS <- c(2:5, 64)

known_essays <- bind_rows(tibble(author = "Hamilton", 
                                 document = HAMILTON_ESSAYS), 
                          tibble(author = "Madison", 
                                 document = MADISON_ESSAYS), 
                          tibble(author = "Jay", 
                                 document = JAY_ESSAYS))

style_words <- tibble(word = c("although", "always", "commonly", "consequently", "considerable", "enough", "there", "upon", "while", "whilst"))

tokens_selected <- corpus_tidy %>% 
  unnest_tokens(word, text) %>% 
  count(document, word) %>%
  group_by(document) %>% 
  mutate(count = n / sum(n) * 1000) %>% 
  select(-n) %>% 
  inner_join(style_words, by = "word") %>% 
  left_join(known_essays, by = "document") %>% 
  spread(word, count, fill = 0) %>% 
  ungroup()

tokens_spread <- tokens_selected %>% 
  filter(!is.na(author)) %>% 
  gather(word, count, -document, -author) %>% 
  group_by(author, word) %>% 
  summarize(avg_count = mean(count)) %>% 
  spread(word, avg_count, fill = 0) %>% 
  filter(author != "Jay") %>% 
  ungroup()

hm_words <- tokens_selected %>% 
  filter(!is.na(author), 
         author != "Jay") %>% 
  mutate(author = ifelse(author == "Hamilton", 1, -1)) 

hm_fit <- lm(author ~ upon + there + consequently + whilst, data = hm_words)
hm_fit

hm_words %>% add_predictions(hm_fit) %>% 
  mutate(pred_author = if_else(pred >= 0, 1, -1)) 
```
```{r}
# predicting authorship of disputed essays
tokens_hm <- tokens_selected %>% 
  filter(!is.na(author), author != "Jay") %>% 
  add_predictions(hm_fit)

UNNOWN_ESSAYS <- c(49, 50:57, 62, 63)
disputed <- tibble(author = "Unknown", 
                   document = UNNOWN_ESSAYS)

tokens_unknown <- corpus_tidy %>% 
  unnest_tokens(word, text) %>% 
  count(document, word) %>%
  group_by(document) %>% 
  mutate(count = n / sum(n) * 1000) %>% 
  select(-n) %>% 
  inner_join(style_words, by = "word") %>% 
  inner_join(disputed, by = "document") %>% 
  spread(word, count, fill = 0) %>% 
  ungroup()

tokens_unknown <- tokens_unknown %>% add_predictions(hm_fit)

tokens_all <- bind_rows(tokens_hm, tokens_unknown)
  
tokens_all %>% 
  ggplot(aes(document, pred, color = author, shape = author)) +
  geom_point(size = 3) +
  labs(x = "Federalist papers", 
       y = "Predicted values") +
  scale_y_continuous(limits = c(-2, 2))
```

```{r}
# Cross-validation; using leave-one-out validation method
crossv_loo <- function(data, id = ".id") {
  modelr::crossv_kfold(data, k = nrow(data), id = id)
}

# leave one out cross-validation object
cv <- hm_words %>%
  crossv_loo()

models <- purrr::map(cv$train, ~ lm(author ~ upon + there + consequently + whilst,
                             data = ., model = FALSE))

test <- map2_df(models, cv$test,
                function(mod, test) {
                  add_predictions(as.data.frame(test), mod) %>%
                    mutate(pred_author =
                             if_else(pred >= 0, "Hamilton", "Madison"),
                           correct = (pred_author == author))
                })

test


  
```

```{r}
## Cross-validation: for loop approach
n <- nrow(hm_words)
hm_classify <- rep(NA, n)
for (i in 1:n) {
  sub_fit <- lm(author ~ upon + there + consequently + whilst, data = hm_words[-i,  ])
  hm_classify[i] <- predict(sub_fit, newdata = hm_words[i,  ])
}
```

```{r}
data("constitution", package = "qss")

tokens_all <- constitution %>% 
  unnest_tokens(word, preamble) %>% 
  anti_join(stop_words, by = "word") %>% 
  mutate(word = str_replace_all(word, "\\d+", "")) %>% 
  filter(word != "")

dtm <- tokens_all %>% count(country, word) 
dtm_tf <- dtm %>% bind_tf_idf(word, country, n)

dtm %>% filter(country == "united_states_of_america") %>% {
  wordcloud(.$word, .$n, max.words = 20) }

dtm_tf %>% filter(country == "united_states_of_america") %>% {
  wordcloud(.$word, .$tf, max.words = 20) }

dtm_tf %>% filter(country == "united_states_of_america") %>% {
  wordcloud(.$word, .$tf_idf, max.words = 20) }
  
```

